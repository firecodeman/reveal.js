<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Hadoop Troubleshooting</title>

		<meta name="description" content="">
		<meta name="author" content="Xiaofei Wang">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="../css/reveal.min.css">
		<link rel="stylesheet" href="../css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="../lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="../css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>
                <style type="text/css">
                  .table{
                    border-left:2px solid white;border-top:2px solid white;
                  }
                  .table td {
                    border-right:2px solid white;border-bottom:2px solid white;
                  }
                </style>

		<!--[if lt IE 9]>
		<script src="../lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Hadoop</h1>
					<h3>TroubleShooting</h3>
					<p>
						<small> by xiaofei</small>
					</p>
				</section>
                                <section>
                                	<section>	
					   <h2>HDFS块信息存储</h2>
					   <img width="50%" height="50%" src="http://hortonworks.com/wp-content/uploads/2011/08/HDFS_Federation_PNG.png"/>
                                        </section>
					<section>
                                           <h2>术语</h2>
                                       	   <h4>Block</h4>
                                             <p>位于NameNode上的Block描述信息</p> 
					   <h4>Replica</h4>
                                             <p>位于DataNode上的副本信息</p>
                                        </section>
					<section>
					  <h2>Block状态</h2>
                                          <ul>
                                            <li>UnderConstruction</li>
                                            <p>被create或append的块,block length 和GS未达到最终值</p>
                                            <li>UnderRecovery</li>
                                            <p>文件lease到期，由状态UnderConstruction转换到此状态</p>
                                            <li>Committed</li>
                                            <p>block的length和GS到达了最终状态.一个未关闭的文件块当NN被新请求一个块时，上一个块由UnderConstruction切换到Committed</p>
                                            <li>Complete</li>
                                            <p>complete的block的length和GS是与各个replica的length和GS是完全匹配的。complete只保留finalized replica的位置</p>
                                          </ul>
					</section>
                                        <section>
                                          <h2>Block状态</h2>
                                          <img src="images/block_status.png"/>
                                        </section>
                                        <section>
                                          <h2>Replica状态</h2>
                                          <ul>
                                            <li>Finalized</li>
                                            <p>finalized replica的字节已经到达最终状态。新的字节只会在做append操作时才再次写入。但finalized replace的GS不会一陈不变，可能会在做recovery后有变化.</p>
                                            <li>Rbw (Replica Being Written to)</li>
                                            <p>replica create或append后，其位于rbw状态。未关闭文件的最后一个块的状态始终是这个。length和GS未达到最终状态 </p>
                                            <li>Rwr (Replica Waiting to be Recovered)</li>
                                            <p>当DataNode死掉或重启时，状态为rbw的replica改为rwr。rwr状态的replica不会出现在pipeline中，也不会接收任何其它数据.</p>
                                          </ul>
                                        </section>
                                        <section>
                                          <h2>Replica状态</h2>
                                          <ul>
                                            <li>rur (Replica Under Recovery)</li>
                                            <p>当lease过期replica将会将状态改为rur</p>
                                            <li>Temporary</li>
                                            <p>temporary状态的replica与replica under construction，但只是由当集群做balance时创建的.它与rwb状态的replica共享很多属性，但数据对用户不可见。在DataNode重启时，位于temporary状态的replica将被删除.</p>
                                          </ul>
                                        </section>
                                        <section>
                                          <h2>Replica状态</h2>
                                          <img src="images/replica_status.png"/>
                                        </section>
                                </section>
                                
                                <section>

                                  <section>
                                    <h2>错误处理</h2>
                                    <ul>
                                      <li>Lease Recovery</li>
                                    </ul>
                                  </section>

                                  <section>
                                    <h2>Lease Recovery</h2>
                                    <ul>
                                      <li>并发控制</li>
                                      <li>一致性保障</li>
                                    </ul>
                                  </section>
                                  <section>
                                    <h2>并发控制</h2>
                                    <p>NN调用 renewLease(由DFSClient 调用rpc触发)改变文件的leaseholder，同时将每次变更持久化到editlog中。如果client的状态是活动状态的，他的所有与写相关的请求都会请求新的generation stamp。如果没有lease holder像new block,close file操作将被拒绝。这可以防止从client端并发的修改未关闭的文件。</p>
                                  </section>
                                  <section>
                                    <h2>一致性保障</h2>
                                    <p>NN会检查文件最后两个block的状态.其它block必须是complete状态。</p>
                                    <!--<table border="0" cellspacing="0" cellpadding="0" class="table">
                                      <tr><td>Penultimate block</td><td>Last block</td><td>Action</td></tr>
                                      <tr><td>Complete</td><td>Complete</td><td><pre>Close the file</pre></td></tr>
                                      <tr><td>Complete</td><td>Committed</td><td rowspan="3"><pre>Retry closing the file when lease expires next time; Force to close the file after a Committed  Complete certain number of retries</pre>
</td></tr>
                                      <tr><td>Committed</td><td>Complete</td><td></td></tr>
                                      <tr><td>Committed</td><td>Committed</td><td></td></tr>
                                      <tr><td>Complete</td><td>UnderConstruction</td><td rowspan="2"><pre>Starts block recovery for the last block</pre></td></tr>
                                      <tr><td>Committed</td><td>UnderConstruction</td><td ></td></tr>
                                      <tr><td>Complete</td><td>UnderRecovery</td><td rowspan="2"><pre>Starts a new block recovery for the last block; stop recovery after a certain number of retries</pre></td></tr>
                                      <tr><td>Committed</td><td>UnderRecovery</td><td ></td></tr>
                                    </table> -->
                                    <img src="images/last_two_block_check.png"/>
                                  </section>
                                </section>

                                <section>
                                   <section>
                                     <h2>Hadoop,HBase错误处理</h2>
                                   </section>
                                   <section>
                                     <h2>DFSClient 持续报 Could not complete file .... retrying...</h2>
                                     <pre>
                                       <code>
java.io.IOException: Bad response ERROR for block BP-178649112-10.35.66.11-1353498632460:blk_-2898024740730493978_16729245 from datanode 10.35.66.21:50010
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:681)
2013-11-18 01:15:44,807 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-178649112-10.35.66.11-1353498632460:blk_-2898024740730493978_1
6729245 in pipeline 10.35.66.54:50010, 10.35.66.17:50010, 10.35.66.21:50010: bad datanode 10.35.66.21:50010
2013-11-18 01:15:50,129 INFO org.apache.hadoop.hdfs.DFSClient: Could not complete file /mapred/system/job_201311171227_0102/job-info retrying...
2013-11-18 01:15:50,531 INFO org.apache.hadoop.hdfs.DFSClient: Could not complete file /mapred/system/job_201311171227_0102/job-info retrying...

                                       </code>
                                     </pre>
                                   </section>
                                   <section>
                                     <h2>DFSClient 持续报 Could not complete file .... retrying...</h2>
                                     <pre>
                                       <code class="java">
//DFSOutputStream
 private void completeFile(ExtendedBlock last) throws IOException {
    long localstart = Time.now();
    boolean fileComplete = false;
    while (!fileComplete) {
      fileComplete = dfsClient.namenode.complete(src, dfsClient.clientName, last);
      if (!fileComplete) {
        if (!dfsClient.clientRunning ||
              (dfsClient.hdfsTimeout > 0 &&
               localstart + dfsClient.hdfsTimeout < Time.now())) {
            String msg = "Unable to close file because dfsclient " +
                          " was unable to contact the HDFS servers." +
                          " clientRunning " + dfsClient.clientRunning +
                          " hdfsTimeout " + dfsClient.hdfsTimeout;
            DFSClient.LOG.info(msg);
            throw new IOException(msg);
        }
        try {
          Thread.sleep(400);
          if (Time.now() - localstart > 5000) {
            DFSClient.LOG.info("Could not complete file " + src + " retrying...");
          }
        } catch (InterruptedException ie) {
        }
      }
    }
  }
                                       </code>
                                     </pre>
                                    </section>
                                    <section>
                                     <h2>DFSClient 持续报 Could not complete file .... retrying...</h2>
                                     <ul>
                                       <li>严重程度:</li>
                                       <p>低</p>
                                       <li>原因:</li>
                                       <p>大批量客户端通过DFSClient调用NameNode中的complete完成块的传输调用rpc超过5秒</p>
                                       <li>解决办法:</li>
                                       <pre><code class="xml">
<!-- hdfs-site.xml -->
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;
		&lt;value&gt;512&lt;/value&gt;
		&lt;final&gt;true&lt;/final&gt;
	&lt;/property&gt;
                                       </code></pre>
                                     </ul>
                                   </section>
                                   
                                   <section>
                                     <h2>NameNode持续报block recover直至NN无响应</h2>
                                     <pre><code>
2013-11-21 18:25:07,534 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Started block recovery for file /data/input/applog/20131119/BAA-OWINMSMQ1_2013111920.log.1384870964955 lease [Lease.  Holder: DFSClient_NONMAPREDUCE_766031715_42, pendingcreates: 1]
2013-11-21 18:25:07,534 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Lease [Lease. Holder: DFSClient_NONMAPREDUCE_766031715_42, pendingcreates: 1] has expired hard limit
2013-11-21 18:25:07,534 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_NONMAPREDUCE_766031715_42, pendingcreates: 1], src=/data/input/applog/20131119/BAA-OWINMSMQ1_2013111920.log.1384870964955
                                     </code></pre>
                                   </section>
                                   <section>
                                     <h2>NameNode持续报block recover直至NN无响应</h2>
                                     <img src="readsource/DFSClient.png"/>
                                   </section>
                                   <section>
                                     <h2>NameNode持续报block recover直至NN无响应</h2>
                                     <img src="readsource/LeaseManager.png"/>
                                   </section>
                                   <section>
                                     <h2>NameNode持续报block recover直至NN无响应</h2>
                                     <pre><code class="java">
//LeaseManager Monitor
  class Monitor implements Runnable {
    final String name = getClass().getSimpleName();

    /** Check leases periodically. */
    @Override
    public void run() {
      for(; shouldRunMonitor && fsnamesystem.isRunning(); ) {
        try {
          fsnamesystem.writeLockInterruptibly();
          try {
            if (!fsnamesystem.isInSafeMode()) {
              checkLeases();
            }
          } finally {
            fsnamesystem.writeUnlock();
          }
  
  
          Thread.sleep(HdfsServerConstants.NAMENODE_LEASE_RECHECK_INTERVAL);
        } catch(InterruptedException ie) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(name + " is interrupted", ie);
          }
        }
      }
    }
  }
                                     </code></pre>


                                   </section>
                                   <section>
                                     <h2>NameNode持续报block recover直至NN无响应</h2>
                                     <pre><code class="java">
//LeaseManager 
  private synchronized void checkLeases() {
    assert fsnamesystem.hasWriteLock();
    for(; sortedLeases.size() > 0; ) {
      final Lease oldest = sortedLeases.first();
      if (!oldest.expiredHardLimit()) {
        return;
      }

      LOG.info("Lease " + oldest + " has expired hard limit");

      final List<String> removing = new ArrayList<String>();
      // need to create a copy of the oldest lease paths, becuase 
      // internalReleaseLease() removes paths corresponding to empty files,
      // i.e. it needs to modify the collection being iterated over
      // causing ConcurrentModificationException
      String[] leasePaths = new String[oldest.getPaths().size()];
      oldest.getPaths().toArray(leasePaths);
      for(String p : leasePaths) {
        try {
          if(fsnamesystem.internalReleaseLease(oldest, p, HdfsServerConstants.NAMENODE_LEASE_HOLDER)) {
            LOG.info("Lease recovery for file " + p +
                          " is complete. File closed.");
            removing.add(p);
          } else {
            LOG.info("Started block recovery for file " + p +
                          " lease " + oldest);
          }
        } catch (IOException e) {
          LOG.error("Cannot release the path "+p+" in the lease "+oldest, e);
          removing.add(p);
        }
      }

      for(String p : removing) {
        removeLease(oldest, p);
      }
    }
  }                                     </code></pre>
                                   </section>
                                   <section>
                                     <h2>NameNode持续报block recover直至NN无响应</h2>
                                     <pre><code class="java">
//FSNameSystem
  private void logReassignLease(String leaseHolder, String src,
      String newHolder) {
    writeLock();
    try {
      getEditLog().logReassignLease(leaseHolder, src, newHolder);
    } finally {
      writeUnlock();
    }
    getEditLog().logSync();
  }
                                     </code></pre>
                                   </section>
                                   <section>
                                     <h2>NameNode持续报block recover直至NN无响应</h2>
                                     <ul>
                                       <li>严重程度:</li>
                                       <p>高</p>
                                       <li>原因:</li>
                                       <p>NN中LeaseManager的Monitor定时检查文件是否硬过期（同时加写锁），如果发现某文件过期则调用FSNameSystem.internalReleaseLease()方法关闭文件，但调用该方法中会触发FSNameSystem.logReassignLease(),同时此方法中也有写锁，造成editlog中的状态不同同步。而interalReleaseLease方法始终返回false,最终功造成死循环。Fix方式见HDFS 4186。</p>
                                     </ul>
                                   </section>
                                   <section>
                                     <h2>NameNode持续报block recover直至NN无响应</h2>
                                       <li>解决办法:</li>
                                     
                                       <ul>
                                          <li>暂时的避免方案是建议在使用DFSClient时及时关闭操作的文件，不要长时间打开着文件，但不写入任何信息，最终造成NameNode Lease硬过期。</li>
                                          <li>长期来看的话需要将当前版本升级到CDH4 4.2.1之后的版本。</li>
                                       </ul>
                                     </ul>
                                   </section>
                                   <section>
                                     <h2>HDFS 4186</h2>
                                     <pre><code class="java">
//LeaseManager
class Monitor implements Runnable {
    final String name = getClass().getSimpleName();

    /** Check leases periodically. */
    @Override
    public void run() {
      for(; shouldRunMonitor && fsnamesystem.isRunning(); ) {
        boolean needSync = false;
        try {
          fsnamesystem.writeLockInterruptibly();
          try {
            if (!fsnamesystem.isInSafeMode()) {
              needSync = checkLeases();
            }
          } finally {
            fsnamesystem.writeUnlock();
            // lease reassignments should to be sync'ed.
            if (needSync) {
              fsnamesystem.getEditLog().logSync();
            }
          }
  
          Thread.sleep(HdfsServerConstants.NAMENODE_LEASE_RECHECK_INTERVAL);
        } catch(InterruptedException ie) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(name + " is interrupted", ie);
          }
        }
      }
    }
  }

  /** Check the leases beginning from the oldest.
   *  @return true is sync is needed.
   */
  private synchronized boolean checkLeases() {
    boolean needSync = false;
    assert fsnamesystem.hasWriteLock();
    for(; sortedLeases.size() > 0; ) {
      final Lease oldest = sortedLeases.first();
      if (!oldest.expiredHardLimit()) {
        return needSync;
      }

      LOG.info(oldest + " has expired hard limit");

      final List<String> removing = new ArrayList<String>();
      // need to create a copy of the oldest lease paths, becuase 
      // internalReleaseLease() removes paths corresponding to empty files,
      // i.e. it needs to modify the collection being iterated over
      // causing ConcurrentModificationException
      String[] leasePaths = new String[oldest.getPaths().size()];
      oldest.getPaths().toArray(leasePaths);
      for(String p : leasePaths) {
        try {
          boolean completed = fsnamesystem.internalReleaseLease(oldest, p,
              HdfsServerConstants.NAMENODE_LEASE_HOLDER);
          if (LOG.isDebugEnabled()) {
            if (completed) {
              LOG.debug("Lease recovery for " + p + " is complete. File closed.");
            } else {
              LOG.debug("Started block recovery " + p + " lease " + oldest);
            }
          }
          // If a lease recovery happened, we need to sync later.
          if (!needSync && !completed) {
            needSync = true;
          }
        } catch (IOException e) {
          LOG.error("Cannot release the path " + p + " in the lease "
              + oldest, e);
          removing.add(p);
        }
      }

      for(String p : removing) {
        removeLease(oldest, p);
      }
    }
    return needSync;
  }
                                     </code></pre>
                                   </section>
                                   

                                   <section>
                                     <h2>HBase RegionServer HLog写入出错，造成RegionServer自动关闭。</h2>
                                     <pre><code>
2013-11-20 01:16:49,124 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Received dynamic protocol exec call with protocolName com.elong.hbase.corproc
essor.GroupByProtocol
2013-11-20 01:17:02,217 WARN org.apache.hadoop.hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-178649112-10.35.66.11-1353498632
460:blk_-4727217747510844304_16938617
java.io.IOException: Bad response ERROR for block BP-178649112-10.35.66.11-1353498632460:blk_-4727217747510844304_16938617 from datanode 10.35.66.51:5001
0
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:681)
2013-11-20 01:17:02,220 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-178649112-10.35.66.11-1353498632460:blk_-4727217747510844304_1
6938617 in pipeline 10.35.66.21:50010, 10.35.66.51:50010, 10.35.66.16:50010: bad datanode 10.35.66.51:50010
2013-11-20 01:17:02,315 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: HDFS pipeline error detected. Found 2 replicas but expecting no less than 3 r
eplicas.  Requesting close of hlog.
2013-11-20 01:17:02,315 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: HLog roll requested
2013-11-20 01:17:02,330 DEBUG org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: using new createWriter -- HADOOP-6840
2013-11-20 01:17:02,330 DEBUG org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Path=hdfs://namenode001.hadoop.bjy.elong.com:9000/hbase/.lo
gs/datanode007.hadoop.bjy.elong.com,60020,1384872281048/datanode007.hadoop.bjy.elong.com%2C60020%2C1384872281048.1384881422315, syncFs=true, hflush=true
2013-11-20 01:17:07,550 INFO org.apache.hadoop.hdfs.DFSClient: Could not complete file /hbase/.logs/datanode007.hadoop.bjy.elong.com,60020,1384872281048/
datanode007.hadoop.bjy.elong.com%2C60020%2C1384872281048.1384881258958 retrying...
                                     </code></pre>
                                   </section>
                                   <section>
                                     <h2>HBase RegionServer HLog写入出错，造成RegionServer自动关闭。</h2>
                                     <ul>
                                       <li>严重程度:</li>
                                       <p>中</p>
                                       <li>问题原因:</li>
                                       <p>dfs.client.block.write.replace-datanode-on-failure.enable开关未开启</p>
                                       <li>解决办法:</li>
                                       <pre><code class="xml">
<!-- hdfs-site.xml -->
        &lt;property&gt;
                &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.enable&lt;/name&gt;
                &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;



                                       </code></pre>
                                     
                                     </ul>
                                   </section>
                                </section>
                                <section>
                                  <h2>END</h2>
                                </section>
                                


			</div> 
		</div>

		<script src="../lib/js/head.min.js"></script>
		<script src="../js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: '../lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: '../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: '../plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: '../plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/leap/leap.js', async: true }
					// { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>
