<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Storm介绍</title>

		<meta name="description" content="Storm Introduction">
		<meta name="author" content="Xiaofei Wang">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="../css/reveal.min.css">
		<link rel="stylesheet" href="../css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="../lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="../css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="../lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Storm</h1>
					<h3>流式数据处理</h3>
					<p>
						<small> by xiaofei</small>
					</p>
				</section>
                                <section>
                                	<section>	
					   <h2>概念</h2>
                                           <p>Storm is a free and open source distributed realtime computation system.</p>
                                        </section>
					<section>
                                           <h2>描述</h2>
                                           <p>Storm makes it easy to reliably process unbounded streams of data, doing for <strong>realtime processing</strong> what Hadoop did for <strong>batch processing</strong>. Storm is simple, can be used with any programming language, and is a lot of fun to use!</p>
                                        </section>
				</section>
				<section>
					<section>
				  		<h2>功能特点</h2>
					</section>
					<section>
						<h2>便于集成</h2>
						<p>Spout已实现与Kestrel、RabbitMQ/AMQP、Kafka、JMS等的集成插件</p>
					</section>
					<section>
						<h2>开发简单易于使用</h2>
						<p>只需要实现Spout、Bolt和topology,将topology提交到集群即可</p>
					</section>
					<section>
						<h2>水平扩展</h2>
						<p>可通过增加机器扩展Spout和Bolt的处理能力</p>
					</section>
					<section>
                                        	<h2>保障数据被处理</h2>
						<ul>
                                        	  <li>记录级容错</li>
                                       		  <li>Transction</li>
						</ul>
                                        </section>
					<section>
						<h2>容错</h2>	
						<ul>
							<li><strong>worker死掉了:</strong>Supervisor将重启它，持续fail且无心跳到Nimbus，Nimbus将重新分配它到其它机器。</li>
							<li><strong>node死掉了:</strong>task分配到的机器无响应或time out，Nimbus将分配到其它机器</li>
							<li><strong>Nimbus或Supervisor服务死掉了:</strong>Nimbus和Supervisor被设计为了无状态的，所有的状态都位于Zookeeper和本地磁盘。只要用daemontools或monit做好进程的监控，失败会就会自动重启。</li>
							<li><strong>Nimbus是否单点(SPOF):</strong>Nimbus失效后不会影响Supervisor监管Worker，但会影响Worker的重新分配。所以这里需要着重增加监管。</li>
						</ul>
					</section>
					<section>
						<h2>易于部署和运维</h2>
						<ul>
							<li>架构类似于MapReduce，便于运维人员维护</li>
						</ul>
					</section>
					<section>
						<h2>兼容多语言</h2>
						<p>Java、Scala、JRuby、Jython、Clojure..</p>
					</section>
                                        <section>
					  <h2>使用场景</h2>
				 	  <ul>
					    <li><img src="images/stream_processing.png"/>流式处理</li>
					    <li><img src="images/dist_rpc.png"/>分布式RPC</li>
					    <li><img src="images/ci_computation.png"/>持续计算</li>

					  </ul>
				          
					</section>
                                </section>
                                

				<section>
					<h2>系统构架</h2>
					<img src="images/storm.png"/>
				</section>
				<section>
					<table style="boder:white soild 1px">
						<thead>
							<tr>
								<td><strong>Hadoop</strong></td><td><strong>Storm</strong></td>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>JobTracker</td><td>Nimbus</td>
							</tr>
							<tr>
								<td></td><td>Zookeeper</td>
							</tr>
							<tr>
								<td>TaskTracker</td><td>Supervisor</td>
							</tr>
							<tr>
								<td>Task</td><td>Worker</td>
							</tr>
						</tbody>
					</table>

				</section>
                                 <section>
					<section>
                                        <h2>编程模型</h2>
                                        <img src="http://storm-project.net/images/topology.png" width="40%" height="40%"/>
					<table style="boder:1px">
                                                <thead>
                                                        <tr>
                                                                <td><strong>Hadoop</strong></td><td><strong>Storm</strong></td>
                                                        </tr>
                                                </thead>
                                                <tbody>
							<tr>
                                                                <td>Job</td><td>Topology</td>
							</tr>
                                                        <tr>
                                                                <td>Map</td><td>Spout</td>

                                                        </tr>
                                                        <tr>
                                                                <td>Reduce</td><td>Bolt</td>
                                                        </tr>
                                                        <tr>
                                                                <td>Writable</td><td>Tuple</td>
							</tr>
                                                        <tr>
                                                                <td>Task(Map/Reduce)</td><td>Task(Spout/Bolt)</td>
                                                        </tr>
                                                </tbody>
                                        </table>
					</section>
					<section>
						<h2>Streams</h2>
						<img src="images/streams.png"/>
					</section>
					<section>
                                                <h2>Spouts</h2>
                                                <img src="images/spouts.png"/>
						<pre>
							<code class="java">
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context,
       SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}
							</code>
						</pre>
                                        </section>
					<section>
                                                <h2>Bolts</h2>
                                                <img src="images/bolts.png"/>
						<ul>
							<li>功能性操作</li>
							<li>过滤操作</li>
							<li>聚集操作</li>
							<li>Join操作</li>
							<li>持久化操作(DB/Cache/HBase/Cassandra)</li>
						</ul>
                                        </section>
					<section>
						<h2>Bolts</h2>
						<pre>
							<code class="java">
public interface IBolt extends Serializable {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    void execute(Tuple input);
    void cleanup();
}

							</code>
						</pre>
					</section>
					<section>
                                                <h2>Topologies</h2>
                                                <img src="http://storm-project.net/images/topology.png" width="60%" height="60%"/>
					</section>
					<section>
						<h2>Topologies</h2>
						<pre>
							<code class="java">
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));
							</code>
						</pre>
                                        </section>
					<section>
                                                <h2>Tasks</h2>
                                                <img src="images/tasks.png"/>
                                        </section>
					<section>
                                                <h2>任务执行</h2>
                                                <img src="images/task_exe1.png"/>
                                        </section>
					<section>
                                                <h2>任务执行</h2>
                                                <img src="images/task_exe2.png"/>
                                        </section>
					 <section>
                                                <h2>数据流分组</h2>
                                                <img src="images/stream_grouping.png"/>
						<ul>
							<li>Shuffle grouping:选择随机任务</li>
							<li>Fields Grouping:在tuple某字段上hash后求模</li>
							<li>All grouping:发送到所有的任务上</li>
							<li>Global grouping:取最小id的任务</li>
							<li>Direct:发送消息时指定任务</li>
							<li>Custorm:用户自定义,实现CustormStreamGrouping</li>
						</ul>
                                        </section>


                                </section>

				<section>
					<section>
						<h2>Word Count</h2>
						<img src="images/word_count.png"/>
					</section>
					<section>
						<h3>WordReader</h3>
						<pre>
							<code class="java">
public class WordReader implements IRichSpout {
  private SpoutOutputCollector collector;
  private FileReader fileReader;
  private boolean completed = false;
  private TopologyContext context;
  public boolean isDistributed() {return false;}
  public void ack(Object msgId) {
    System.out.println("OK:"+msgId);
  }
  public void close() {}
  public void fail(Object msgId) {
    System.out.println("FAIL:"+msgId);
  }
  /**
  * The only thing that the methods will do It is emit each
  * file line
  */
  public void nextTuple() {
    /**
    * The nextuple it is called forever, so if we have been readed the file
    * we will wait and then return
    */
    if(completed){
      try {
        Thread.sleep(1000);
      } catch (InterruptedException e) {
      //Do nothing
      }
      return;
    }
    String str;
    //Open the reader
    BufferedReader reader = new BufferedReader(fileReader);
    try{
      //Read all lines
      while((str = reader.readLine()) != null){
        /**
        * By each line emmit a new value with the line as a their
        */
        this.collector.emit(new Values(str),str);
      }
    }catch(Exception e){
      throw new RuntimeException("Error reading tuple",e);
    }finally{
      completed = true;

    }
  }

  /**
  * We will create the file and get the collector object
  */
  public void open(Map conf, TopologyContext context,SpoutOutputCollector collector) {
    try {
      this.context = context;
      this.fileReader = new FileReader(conf.get("wordsFile").toString());
    } catch (FileNotFoundException e) {
      throw new RuntimeException("Error reading file["+conf.get("wordFile")+"]");
    }
    this.collector = collector;
  }

  /**
  * Declare the output field "word"
  */
  public void declareOutputFields(OutputFieldsDeclarer declarer) {
    declarer.declare(new Fields("line"));
  }
}

				
							</code>
						</pre>
					</section>
					<section>
						<h3>WordNormalizer</h3>
						<pre>
							<code class="java">
public class WordNormalizer implements IRichBolt {
  private OutputCollector collector;
  public void cleanup() {}
  /**
  * The bolt will receive the line from the
  * words file and process it to Normalize this line
  *
  * The normalize will be put the words in lower case
  * and split the line to get all words in this
  */
  public void execute(Tuple input) {
    String sentence = input.getString(0);
    String[] words = sentence.split(" ");
    for(String word : words){
      word = word.trim();
      if(!word.isEmpty()){
        word = word.toLowerCase();
        //Emit the word
        List a = new ArrayList();
        a.add(input);
        collector.emit(a,new Values(word));
      }
    }
    // Acknowledge the tuple
    collector.ack(input);
  }

  public void prepare(Map stormConf, TopologyContext context,OutputCollector collector) {
    this.collector = collector;
  }

  /**
  * The bolt will only emit the field "word"
  */
  public void declareOutputFields(OutputFieldsDeclarer declarer) {
    declarer.declare(new Fields("word"));
  }
}

		
							</code>
						</pre>
					</section>
					<section>
						<h3>WordCounter</h3>
						<pre>
							<code class="java">
public class WordCounter implements IRichBolt {
  Integer id;
  String name;
  Map<String, Integer> counters;
  private OutputCollector collector;

  /**
  * At the end of the spout (when the cluster is shutdown
  * We will show the word counters
  */
  @Override
  public void cleanup() {
    System.out.println("-- Word Counter ["+name+"-"+id+"] --");
    for(Map.Entry<String, Integer> entry : counters.entrySet()){
      System.out.println(entry.getKey()+": "+entry.getValue());
    }
  }

  /**
  * On each word We will count
  */
  @Override
  public void execute(Tuple input) {
    String str = input.getString(0);
  /**
  * If the word dosn't exist in the map we will create
  * this, if not We will add 1
  */
    if(!counters.containsKey(str)){
      counters.put(str, 1);
    }else{
      Integer c = counters.get(str) + 1;
      counters.put(str, c);
    }
    //Set the tuple as Acknowledge
    collector.ack(input);
  }

  /**
  * On create
  */
  @Override
  public void prepare(Map stormConf, TopologyContext context,OutputCollector collector) {
    this.counters = new HashMap<String, Integer>();
    this.collector = collector;
    this.name = context.getThisComponentId();
    this.id = context.getThisTaskId();
  }

  @Override
  public void declareOutputFields(OutputFieldsDeclarer declarer) {}
}

							</code>
						</pre>
					</section>
				</section>
				<section>
					<h2>DRPC</h2>
					<img src="images/drpc.png"/>
				</section>
				<section>
					<h2>记录级容错</h2>
					<img src="https://github.com/nathanmarz/storm/wiki/images/tuple_tree.png"/>
					
				</section>
				
				<section>
					<h2>记录级容错</h2>
					<ul>
						<li>Tuple超过topology.message.timeout.secs未被处理被视为失效</li>
						<li>首先Storm从Spout中请求nextTuple获取tuple</li>
						<li>Spout通过SpoutOutputCollector提供的open方法输出tuple到输出流,并提供一个message id标识此tuple</li>
						<li>接着tuple被Bolt消费,Storm跟踪它创建的消息树。如果Storm检测到tuple被fully process，Storm调用ack传入该message id。如果tuple超时将在Spout中调用fail方法。</li>
				
					</ul>
				</section>
				<!--<section>
					<h2>Storm 可靠性 API</h2>
					<ul>
						<li>首先,你需要告诉Storm是否你在tuple tree中创建了新的连接</li>
						<li>另外,当你的额外的tuple完成时告诉Storm</li>
					</ul>
				</section>-->
				<section>
					<h2>事务拓扑</h2>
					<ul>
						<li>Transactional topology</li>
						<li>Trident</li>
					</ul>
				</section>
				<section>
					<h2>Transactional topology</h2>
					<ul>
						<li>0.7.0引入</li>
						<li>需要支持指定批次replay的queue</li>
						<li>实现了基于kafka的transactional spout</li>
					</ul>
				</section>
				<section>
					<h2>Transactional topology</h2>
					<img src="images/transactional_topology.png"/>
				</section>
				<section>
					<h2>Trident</h2>
					<ul>
						<li>0.8.0引入</li>
					</ul>
					<img src="https://github.com/nathanmarz/storm/wiki/images/batched-stream.png"/>
				</section>
				<section>
					<h2>Trident Spout</h2>
					<pre>
						<code class="java">
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"),
spout.setCycle(true);
						</code>
					</pre>
					
				</section>
				<section>
					<h2>Trident Topology</h2>
					<pre>
						<code class="java">
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);
						</code>
					</pre>
				</section>
				<section>
					<h2>Trident BaseFunction</h2>
					<pre>
						<code class="java">
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}
						</code>
					</pre>
				</section>
				<section>
					<h2>Trident DRPC</h2>
					<pre>
						<code class="java">
DRPCClient client = new DRPCClient("drpc.server.location", 3772);
System.out.println(client.execute("words", "cat dog the man");
// prints the JSON-encoded result, e.g.: "[[5078]]"
						</code>
					</pre>
					<pre>
						<code class="java">
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));
						</code>
					</pre>
				</section>
				<section>
					<h2>状态存储</h2>
					<ul>
						<li>Trident解决的两个问题</li>
					<ul>
						<li>为每一批次提供了一个唯一的transaction id，重试时使用一致的transaction id</li>
						<li>状态更新依据批次的顺序</li>
						
					</ul>
						<li>存储方式</li>
					<ul>
						<li>Memory</li>
						<li>Memcached</li>
						<li>Cassandra</li>
						<li>...</li>
					</ul>
					</ul>
					
				</section>
				<section>
					<h2>Trident topology</h2>
					<img src="https://github.com/nathanmarz/storm/wiki/images/trident-to-storm2.png"/>
				</section>
				<section>
					<h2>Storm的问题</h2>
					<ul>
						<li>向topology增加业务时需要删除原有topology部署新的topology(0.9增加了swap)</li>
						<li>topology间不能传递数据（只能通过Queue队列间接传递)</li>
					</ul>
				</section>
				<section>
					<section>
						<h2>业界应用</h2>
					</section>
					<section>
						<h2>Twitter场景(Tweitgeist)</h2>
						<h6>Live top 10 trending hashtags on Twitter</h6>
						<img src="images/twitter_example.png"/>
						
					</section>
					<section>
						<h2>一淘数据部(月光宝盒)</h2>
						<img src="images/taobao_example.png" width="70%" height="70%"/>
					</section>
					<section>
                                                <h2>一淘数据部(月光宝盒)</h2>
                                                <img src="images/taobao_example_1.png" width="70%" height="70%"/>
                                        </section>
					<section>
						<h2>携程(Alerting)</h2>
						<img src="images/crip_example.png" width="70%" height="70%"/>
					</section>
					
				</section>

				<section>
					<section>
						<h2>运维监控</h2>
					</section>
					<section>
						<h2>监控工具</h2>
						<ul>
							<li>Storm UI</li>
							<li>JMX/VisualVM</li>
							<li>Yammer Metrics</li>
							<li>Ganglia/Graphite</li>
							<li>Log4j</li>
							<li>Nagios(监控硬件、系统、log中的ERROR/WARN)</li>
							<li>OpenTSDB</li>
							<li>storm-monitor</li>
						</ul>
					</section>
					<section>
						<h2>Storm UI</h2>
						<img src="images/storm_ui.png" width="80%" height="80%"/>
					</section>
					<section>
						<h2>JMX/VisualVM</li>
						<img src="images/storm_jmx.png" width="80%" height="80%"/>
						<!--<pre>
#storm.xml							<code>–
-worker.childopts: ”
  -Dcom.sun.management.jmxremote
  -Dcom.sun.management.jmxremote.ssl=false
  -Dcom.sun.management.jmxremote.authenticate=false
  -Dcom.sun.management.jmxremote.local.only=false
  -Dcom.sun.management.jmxremote.port=1%ID%”
</code>
						</pre>-->
					</section>
					<section>
						<h2>Yammer Metrics</h2>
						<h5>http://metrics.codahale.com/</h5>
						<pre>
							<code class="xml">
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.codahale.metrics&lt;/groupId&gt;
        &lt;artifactId&gt;metrics-core&lt;/artifactId&gt;
        &lt;version&gt;${metrics.version}&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
							</code>
						</pre>
					</section>
					<section>
                                                <h2>Yammer Metrics</h2>
                                                <pre>
                                                        <code class="java">
//application level static 
final MetricRegistry metrics = new MetricRegistry();
//Gauges 标尺
public class QueueManager {
    private final Queue queue;

    public QueueManager(MetricRegistry metrics, String name) {
        this.queue = new Queue();
        metrics.register(MetricRegistry.name(QueueManager.class, name, "size"),
                         new Gauge<Integer>() {
                             @Override
                             public Integer getValue() {
                                 return queue.size();
                             }
                         });
    }
}


//counters 计数器
private final Counter pendingJobs = metrics.counter(name(QueueManager.class, "pending-jobs"));

public void addJob(Job job) {
    pendingJobs.inc();
    queue.offer(job);
}

public Job takeJob() {
    pendingJobs.dec();
    return queue.take();
}

..
                                                        </code>
                                                </pre>
                                        </section>
					<section>
						<h2>Ganglia</h2>
						<img src="images/ganglia.png"/>
					</section>
					<section>
                                                <h2>Graphite</h2>
						<img src="http://graphite.wikidot.com/local--files/screen-shots/graphite_fullscreen_800.png"/>
						
                                        </section>
					<section>
						<h2>OpenTSDB</h2>
						<img src="http://opentsdb.net/img/tsdb-architecture.png"/>
					</section>	
					<section>
						<img src="http://opentsdb.net/img/tsd-sample.png"/>
					</section>

					<section>
						<h2>storm-monitor</h2>
						<h5>https://github.com/killme2008/storm-monitor参考用于监控Nimbus</h5>
						<ul>
							<li>监控supervisor数目是否正确，当supervisor挂掉的时候会发送警告</li>
							<li>监控nimbus是否正常运行，monitor会尝试连接nimbus，如果连接失败就认为nimbus挂掉。</li>
							<li>监控topology是否正常运行，包括它是否正常部署，是否有运行中的任务</li>
						</ul>
	
					</section>
					<section>
						<h2>其它可选监控工具</h2>
						<ul>
							<li>Ooyala metrics_storm</li>
							<li>Storm 0.9 Metrics</li>
							<ul>
								<li>按固定的时间窗口收集任意的用户自定义metric</li>
								<li>metric最终被Storm聚合</li>
								<li>可自定义MetricsConsumer</li>
							</ul>
						</ul>
					</section>
					<section>
						<h2>服务性能监控</h2>
					<ul>
						<li>拓扑图</li>
						<ul>
							<li>吞吐量</li>
							<li>延迟</li>
						</ul>
						<li>Bolt和spout</li>
						<ul>
							<li>吞吐量</li>
							<li>延迟</li>
							<li>队列时间</li>
							<li>执行时间</li>
						</ul>
						<li>数据来源和数据存储</li>
						<ul>
							<li>读/写条数</li>
							<li>读/写速度</li>
						</ul>
						<li>Storm 0.8.2 new metrics</li>
						<ul>
							<li>process latency(tuple直到被ack的时间)</li>
							<li>execute latency(tuple被调度执行的时间)</li>
							<li>capacity(上十分钟bolt用于执行tuple的时间百分比)</li>
						</ul>
					</ul>
					</section>
					<section>
						<h2>CheckList应用级监控</h2>
						<ul>
							<li>Kafka读取速度</li>
							<li>HBase写入速度</li>
							<li>规则处理引擎某topic单条数据处理时间</li>
							<li>规则处理引擎处理速度</li>
							<li>规则处理引擎出错次数</li>
							<li>处理数据延迟(根据数据业务日期)</li>
							<li>QueryTools查询响应时间</li>
							<li>数据接口监控</li>
						</ul>
					</section>
					<!--<section>
						<h2>问题定位</h2>	
						<ul>
							<li>是吞吐量下降？</li>
							<li>跟踪延迟增长</li>
							<li>检查JVM(memory,GC)问题</li>
							<li>检查JVM(memory,GC)问题</li>
						</ul>
					</section>-->
					<section>
						<h2>推荐监控方案</h2>
						<ul>
							<li>服务或组件监控</li>
							<ul>
								<li>Nagios(硬件、服务进程、log)参考storm_monitor</li>
								<li>Storm UI(Thrift)</li>
								<li>Ooyala metrics_storm</li>
								<li>Ganglia/Graphite</li>
							</ul>
							<li>应用监控</li>
							<ul>
								<li>Yammer Metrics</li>
							</ul>
						</ul>
					</section>
					<section>
						<h2>网络规划</h2>
						<img src="images/network_topology.png"/>
					</section>
				</section>

				<section>
					<h1>THE END</h1>
				</section>

			</div>

		</div>

		<script src="../lib/js/head.min.js"></script>
		<script src="../js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: '../lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: '../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: '../plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: '../plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/leap/leap.js', async: true }
					// { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>
